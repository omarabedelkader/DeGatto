{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2690c26c",
   "metadata": {},
   "source": [
    "# 1) LIBRARIES NEEDED"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2da6fa23",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\omarm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\omarm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\omarm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\omarm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\omarm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\omarm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import re\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import scipy.stats as stats\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "import missingno as msno \n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "from sklearn.metrics import plot_confusion_matrix, classification_report, f1_score, recall_score\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "import nltk\n",
    "nltk.download(\"punkt\")\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "# Importing plotly and cufflinks in offline mode\n",
    "import plotly.express as px\n",
    "import plotly.offline\n",
    "\n",
    "# Ignore Warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.warn(\"this will not show\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "058f324a",
   "metadata": {},
   "source": [
    "# 2- Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b80e2eb",
   "metadata": {},
   "source": [
    "## 2.2) About the Features"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decb21a1",
   "metadata": {},
   "source": [
    "1) Clothing ID: Integer Categorical variable that refers to the specific piece being reviewed.\n",
    "\n",
    "2) Review Text: String variable for the review body.\n",
    "\n",
    "3) Rating: Positive Ordinal Integer variable for the product score granted by the customer from 1 Worst, to 5 Best.\n",
    "\n",
    "4) Recommended IND: Binary variable stating where the customer recommends the product where 1 is recommended, 0 is not recommended.\n",
    "\n",
    "5) Category Name: Categorical name of the product department name.\n",
    "\n",
    "6) Sentiment:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21ff8465",
   "metadata": {},
   "source": [
    "# 3) Reading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "73927855",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Clothing ID</th>\n",
       "      <th>Review_Text</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Recommended_IND</th>\n",
       "      <th>Category_name</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Size</th>\n",
       "      <th>Material</th>\n",
       "      <th>Longevity</th>\n",
       "      <th>Color</th>\n",
       "      <th>General</th>\n",
       "      <th>Comfort</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>96</td>\n",
       "      <td>861</td>\n",
       "      <td>i am in need of easy comfortable tops for ever...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Knits</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>POS</td>\n",
       "      <td>POS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>123</td>\n",
       "      <td>1133</td>\n",
       "      <td>i read the previous reviews and had hoped that...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Jackets</td>\n",
       "      <td>Positive</td>\n",
       "      <td>POS</td>\n",
       "      <td>POS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>240</td>\n",
       "      <td>872</td>\n",
       "      <td>this is exactly what i was expecting cute comf...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Knits</td>\n",
       "      <td>Positive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>276</td>\n",
       "      <td>1104</td>\n",
       "      <td>this dress is gorgeous i love it i bought it t...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Positive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>310</td>\n",
       "      <td>836</td>\n",
       "      <td>love this top made with 100 cotton a vintage l...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Positive</td>\n",
       "      <td>POS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Clothing ID                                        Review_Text  \\\n",
       "0          96          861  i am in need of easy comfortable tops for ever...   \n",
       "1         123         1133  i read the previous reviews and had hoped that...   \n",
       "2         240          872  this is exactly what i was expecting cute comf...   \n",
       "3         276         1104  this dress is gorgeous i love it i bought it t...   \n",
       "4         310          836  love this top made with 100 cotton a vintage l...   \n",
       "\n",
       "   Rating  Recommended_IND Category_name Sentiment Size Material Longevity  \\\n",
       "0       3                0         Knits   Neutral  POS      POS       NaN   \n",
       "1       4                1       Jackets  Positive  POS      POS       NaN   \n",
       "2       5                1         Knits  Positive  NaN      POS       NaN   \n",
       "3       5                1       Dresses  Positive  NaN      POS       NaN   \n",
       "4       5                1          Tops  Positive  POS      NaN       NaN   \n",
       "\n",
       "  Color General Comfort  \n",
       "0   POS     NaN     POS  \n",
       "1   POS     NaN     POS  \n",
       "2   POS     NaN     POS  \n",
       "3   POS     NaN     POS  \n",
       "4   POS     NaN     POS  "
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = pd.read_excel(\"Output Womens Clothing E-Commerce Reviews.xlsx\")\n",
    "data_copy = data.copy()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd82fd1",
   "metadata": {},
   "source": [
    "# 4) Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "04112921",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 23467 entries, 0 to 23466\n",
      "Data columns (total 13 columns):\n",
      " #   Column           Non-Null Count  Dtype \n",
      "---  ------           --------------  ----- \n",
      " 0   Unnamed: 0       23467 non-null  int64 \n",
      " 1   Clothing ID      23467 non-null  int64 \n",
      " 2   Review_Text      22641 non-null  object\n",
      " 3   Rating           23467 non-null  int64 \n",
      " 4   Recommended_IND  23467 non-null  int64 \n",
      " 5   Category_name    23467 non-null  object\n",
      " 6   Sentiment        23467 non-null  object\n",
      " 7   Size             3485 non-null   object\n",
      " 8   Material         2703 non-null   object\n",
      " 9   Longevity        57 non-null     object\n",
      " 10  Color            2933 non-null   object\n",
      " 11  General          302 non-null    object\n",
      " 12  Comfort          561 non-null    object\n",
      "dtypes: int64(4), object(9)\n",
      "memory usage: 2.3+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c13f1e63",
   "metadata": {},
   "source": [
    "# 5) FEATURE SELECTION & DATA CLEANING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22fb3426",
   "metadata": {},
   "source": [
    "## 5.1 Feature Selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "29e87fa5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Clothing ID</th>\n",
       "      <th>Review_Text</th>\n",
       "      <th>Rating</th>\n",
       "      <th>Recommended_IND</th>\n",
       "      <th>Category_name</th>\n",
       "      <th>Sentiment</th>\n",
       "      <th>Size</th>\n",
       "      <th>Material</th>\n",
       "      <th>Longevity</th>\n",
       "      <th>Color</th>\n",
       "      <th>General</th>\n",
       "      <th>Comfort</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>96</td>\n",
       "      <td>861</td>\n",
       "      <td>i am in need of easy comfortable tops for ever...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>Knits</td>\n",
       "      <td>Neutral</td>\n",
       "      <td>POS</td>\n",
       "      <td>POS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>123</td>\n",
       "      <td>1133</td>\n",
       "      <td>i read the previous reviews and had hoped that...</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>Jackets</td>\n",
       "      <td>Positive</td>\n",
       "      <td>POS</td>\n",
       "      <td>POS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>240</td>\n",
       "      <td>872</td>\n",
       "      <td>this is exactly what i was expecting cute comf...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Knits</td>\n",
       "      <td>Positive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>276</td>\n",
       "      <td>1104</td>\n",
       "      <td>this dress is gorgeous i love it i bought it t...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Dresses</td>\n",
       "      <td>Positive</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>310</td>\n",
       "      <td>836</td>\n",
       "      <td>love this top made with 100 cotton a vintage l...</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>Tops</td>\n",
       "      <td>Positive</td>\n",
       "      <td>POS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POS</td>\n",
       "      <td>NaN</td>\n",
       "      <td>POS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Clothing ID                                        Review_Text  \\\n",
       "0          96          861  i am in need of easy comfortable tops for ever...   \n",
       "1         123         1133  i read the previous reviews and had hoped that...   \n",
       "2         240          872  this is exactly what i was expecting cute comf...   \n",
       "3         276         1104  this dress is gorgeous i love it i bought it t...   \n",
       "4         310          836  love this top made with 100 cotton a vintage l...   \n",
       "\n",
       "   Rating  Recommended_IND Category_name Sentiment Size Material Longevity  \\\n",
       "0       3                0         Knits   Neutral  POS      POS       NaN   \n",
       "1       4                1       Jackets  Positive  POS      POS       NaN   \n",
       "2       5                1         Knits  Positive  NaN      POS       NaN   \n",
       "3       5                1       Dresses  Positive  NaN      POS       NaN   \n",
       "4       5                1          Tops  Positive  POS      NaN       NaN   \n",
       "\n",
       "  Color General Comfort  \n",
       "0   POS     NaN     POS  \n",
       "1   POS     NaN     POS  \n",
       "2   POS     NaN     POS  \n",
       "3   POS     NaN     POS  \n",
       "4   POS     NaN     POS  "
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "52b43efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.drop(['Unnamed: 0', 'Clothing ID', 'Rating','Recommended_IND', 'Category_name' , 'Size', 'Material', 'Longevity', 'Color', 'General', 'Comfort' ], axis =1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "bc987075",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review_Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i am in need of easy comfortable tops for ever...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i read the previous reviews and had hoped that...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this is exactly what i was expecting cute comf...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this dress is gorgeous i love it i bought it t...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>love this top made with 100 cotton a vintage l...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Review_Text Sentiment\n",
       "0  i am in need of easy comfortable tops for ever...   Neutral\n",
       "1  i read the previous reviews and had hoped that...  Positive\n",
       "2  this is exactly what i was expecting cute comf...  Positive\n",
       "3  this dress is gorgeous i love it i bought it t...  Positive\n",
       "4  love this top made with 100 cotton a vintage l...  Positive"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f1c7e0f",
   "metadata": {},
   "source": [
    "## 5.2 Detecting & Handling With Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "94b1e93e",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (232203466.py, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\omarm\\AppData\\Local\\Temp\\ipykernel_20564\\232203466.py\"\u001b[1;36m, line \u001b[1;32m2\u001b[0m\n\u001b[1;33m    print(\"Missing Value for Features 'Review_Text' \")\u001b[0m\n\u001b[1;37m    ^\u001b[0m\n\u001b[1;31mIndentationError\u001b[0m\u001b[1;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "#def miss_val():\n",
    "    print(\"Missing Value for Features 'Review_Text' \")\n",
    "    print(data['Review_Text'].isnull().value_counts())\n",
    "    print(\"Missing Value for Features 'Sentiment' \")\n",
    "    print(data['Sentiment'].isnull().value_counts())\n",
    "    print(\"Missing Value for Features 'Size' \")\n",
    "    print(data['Size'].isnull().value_counts())\n",
    "    print(\"Missing Value for Features 'Material' \")\n",
    "    print(data['Material'].isnull().value_counts())\n",
    "    print(\"Missing Value for Features 'Longevity' \")\n",
    "    print(data['Longevity'].isnull().value_counts())\n",
    "    print(\"Missing Value for Features 'Color' \")\n",
    "    print(data['Color'].isnull().value_counts())\n",
    "    print(\"Missing Value for Features 'General' \")\n",
    "    print(data['General'].isnull().value_counts())\n",
    "    print(\"Missing Value for Features 'Comfort' \")\n",
    "    print(data['Comfort'].isnull().value_counts())\n",
    "    \n",
    "miss_val()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "58a836f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review_Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i am in need of easy comfortable tops for ever...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i read the previous reviews and had hoped that...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this is exactly what i was expecting cute comf...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this dress is gorgeous i love it i bought it t...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>love this top made with 100 cotton a vintage l...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Review_Text Sentiment\n",
       "0  i am in need of easy comfortable tops for ever...   Neutral\n",
       "1  i read the previous reviews and had hoped that...  Positive\n",
       "2  this is exactly what i was expecting cute comf...  Positive\n",
       "3  this dress is gorgeous i love it i bought it t...  Positive\n",
       "4  love this top made with 100 cotton a vintage l...  Positive"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "901b3256",
   "metadata": {},
   "source": [
    "# 6) TEXT MINING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "862b85d9",
   "metadata": {},
   "source": [
    "OverSampling & UnderSampling\n",
    "\n",
    "Remove Punctuation\n",
    "\n",
    "Lowercase\n",
    "\n",
    "Tokenization\n",
    "\n",
    "Stopwords (We will try both ways remove and let them stay)\n",
    "\n",
    "Lemmatization / Stemming\n",
    "\n",
    "Remove numbers\n",
    "\n",
    "Remove pronouns (Check if needed to remove)\n",
    "\n",
    "Remove consecutive repeating words (to see if it should be done)\n",
    "\n",
    "Vectorization\n",
    "\n",
    "TF-IDF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5963eecf",
   "metadata": {},
   "source": [
    "## Tokenization, Noise Removal & Lexicon Normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "862d4dc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cleaning_fsa(data):\n",
    "    \n",
    "    import re\n",
    "    #1. Remove Puncs\n",
    "    # \\w typically matches [A-Za-z0-9_]\n",
    "    text = re.sub('[A-Za-z0-9_]','', data)\n",
    "         \n",
    "    #2. Tokenize\n",
    "    text_tokens = word_tokenize(text.lower()) \n",
    "    \n",
    "    #3. Remove numbers\n",
    "    tokens_without_punc = [w for w in text_tokens if w.isalpha()]\n",
    "    \n",
    "    #4. Removing Stopwords\n",
    "    tokens_without_sw = [t for t in tokens_without_punc if t not in stop_words]\n",
    "    \n",
    "    #5. lemma\n",
    "    text_cleaned = [WordNetLemmatizer().lemmatize(t) for t in tokens_without_sw]\n",
    "    \n",
    "    #joining\n",
    "    return \" \".join(text_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a5c5947",
   "metadata": {},
   "source": [
    "## 7) SENTIMENT CLASSIFICATION WITH MACHINE LEARNING & DEEP LEARNING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f7752c6",
   "metadata": {},
   "source": [
    "We will directly work on ACCURACY, RECALL, PRECISION, F1-SCORE and take one of them NOT NECESSIRALY accuracy, depending on the best fitted, and the sampling of the data\n",
    "\n",
    "Naive Bayes\n",
    "Logistic Regression\n",
    "SVM (different ones, polynomial, linear, SVC, ...)\n",
    "LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ef08a06",
   "metadata": {},
   "source": [
    "## 7.1 Train | Test & Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "594f9666",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = data[\"Review_Text\"]\n",
    "y= data[\"Sentiment\"]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=101)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "8b8bf8d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = data.fillna(' '), data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "7f7e4610",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Review_Text</th>\n",
       "      <th>Sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>i am in need of easy comfortable tops for ever...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i read the previous reviews and had hoped that...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this is exactly what i was expecting cute comf...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this dress is gorgeous i love it i bought it t...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>love this top made with 100 cotton a vintage l...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         Review_Text Sentiment\n",
       "0  i am in need of easy comfortable tops for ever...   Neutral\n",
       "1  i read the previous reviews and had hoped that...  Positive\n",
       "2  this is exactly what i was expecting cute comf...  Positive\n",
       "3  this dress is gorgeous i love it i bought it t...  Positive\n",
       "4  love this top made with 100 cotton a vintage l...  Positive"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "59c01dfb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "826"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Review_Text'].isnull().values.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "7d46bae2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        i am in need of easy comfortable tops for ever...\n",
       "1        i read the previous reviews and had hoped that...\n",
       "2        this is exactly what i was expecting cute comf...\n",
       "3        this dress is gorgeous i love it i bought it t...\n",
       "4        love this top made with 100 cotton a vintage l...\n",
       "                               ...                        \n",
       "23462    i was very happy to snag this dress at such a ...\n",
       "23463    it reminds me of maternity clothes soft stretc...\n",
       "23464    this fit well but the top was very see through...\n",
       "23465    i bought this dress for a wedding i have this ...\n",
       "23466    this dress in a lovely platinum is feminine an...\n",
       "Name: Review_Text, Length: 22641, dtype: object"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Review_Text'].dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "b4eba579",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Review_Text'].isnull().values.any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "c3da3d88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0         Neutral\n",
       "1        Positive\n",
       "2        Positive\n",
       "3        Positive\n",
       "4        Positive\n",
       "           ...   \n",
       "23462    Positive\n",
       "23463     Neutral\n",
       "23464     Neutral\n",
       "23465     Neutral\n",
       "23466    Positive\n",
       "Name: Sentiment, Length: 23467, dtype: object"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Sentiment'].dropna()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d56184",
   "metadata": {},
   "source": [
    "## 7.2 Vectorization"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85958ca3",
   "metadata": {},
   "source": [
    "## 7.2.a Count Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "e21cf129",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'CONTENT'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20564\\2302587407.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCONTENT\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfillna\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m' '\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msentiment\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   5573\u001b[0m         ):\n\u001b[0;32m   5574\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 5575\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mobject\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__getattribute__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   5576\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   5577\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__setattr__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m->\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'DataFrame' object has no attribute 'CONTENT'"
     ]
    }
   ],
   "source": [
    "X, y = data.CONTENT.fillna(' '), data.sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "ef955163",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>i am in need of easy comfortable tops for everyday wear i bought this top mostly because of the cute buttons when i received it it looked exactly as it does in the picture online however the buttons kept slipping out of their homes because the holes were slightly too big the shirt fit but was just a tad snug near the upper arms which would stretch and loosen up throughout the day it s definitely a comfortable shirt but it felt more like a pajama top it s going back</th>\n",
       "      <th>Neutral</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>i read the previous reviews and had hoped that...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>this is exactly what i was expecting cute comf...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>this dress is gorgeous i love it i bought it t...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>love this top made with 100 cotton a vintage l...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>soft comfortable stylish i eagerly awaited the...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23462</th>\n",
       "      <td>i was very happy to snag this dress at such a ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23463</th>\n",
       "      <td>it reminds me of maternity clothes soft stretc...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23464</th>\n",
       "      <td>this fit well but the top was very see through...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23465</th>\n",
       "      <td>i bought this dress for a wedding i have this ...</td>\n",
       "      <td>Neutral</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23466</th>\n",
       "      <td>this dress in a lovely platinum is feminine an...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>23466 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "0     i am in need of easy comfortable tops for everyday wear i bought this top mostly because of the cute buttons when i received it it looked exactly as it does in the picture online however the buttons kept slipping out of their homes because the holes were slightly too big the shirt fit but was just a tad snug near the upper arms which would stretch and loosen up throughout the day it s definitely a comfortable shirt but it felt more like a pajama top it s going back   \\\n",
       "1      i read the previous reviews and had hoped that...                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
       "2      this is exactly what i was expecting cute comf...                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
       "3      this dress is gorgeous i love it i bought it t...                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
       "4      love this top made with 100 cotton a vintage l...                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
       "5      soft comfortable stylish i eagerly awaited the...                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
       "...                                                  ...                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
       "23462  i was very happy to snag this dress at such a ...                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
       "23463  it reminds me of maternity clothes soft stretc...                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
       "23464  this fit well but the top was very see through...                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
       "23465  i bought this dress for a wedding i have this ...                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
       "23466  this dress in a lovely platinum is feminine an...                                                                                                                                                                                                                                                                                                                                                                                                                                       \n",
       "\n",
       "0       Neutral  \n",
       "1      Positive  \n",
       "2      Positive  \n",
       "3      Positive  \n",
       "4      Positive  \n",
       "5      Positive  \n",
       "...         ...  \n",
       "23462  Positive  \n",
       "23463   Neutral  \n",
       "23464   Neutral  \n",
       "23465   Neutral  \n",
       "23466  Positive  \n",
       "\n",
       "[23466 rows x 2 columns]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.columns = X.iloc[0]\n",
    "X.drop(X.index[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "5674295b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20564\\4134487124.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mX_train_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mX_test_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1328\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1330\u001b[1;33m         \u001b[0mvocabulary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1332\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1199\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1200\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1201\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1202\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1203\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdecoder\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0manalyzer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 226\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m    227\u001b[0m                 \u001b[1;34m\"np.nan is an invalid document, expected byte or unicode string.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m             )\n",
      "\u001b[1;31mValueError\u001b[0m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "X_train_count = vectorizer.fit_transform(X_train)\n",
    "X_test_count = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d44a5781",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_count' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20564\\3909002473.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train_count\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_count' is not defined"
     ]
    }
   ],
   "source": [
    "type(X_train_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8066035e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_count.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff4ebe6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8386f02d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train_count.toarray(), columns = vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ca83ae9",
   "metadata": {},
   "source": [
    "### 7.2.b TF-IDF Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "c113a571",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "np.nan is an invalid document, expected byte or unicode string.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_20564\\3990982114.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mtf_idf_vectorizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTfidfVectorizer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mX_train_tf_idf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_idf_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mX_test_tf_idf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf_idf_vectorizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   2075\u001b[0m         \"\"\"\n\u001b[0;32m   2076\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2077\u001b[1;33m         \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit_transform\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2078\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_tfidf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2079\u001b[0m         \u001b[1;31m# X is already a transformed view of raw_documents so\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mfit_transform\u001b[1;34m(self, raw_documents, y)\u001b[0m\n\u001b[0;32m   1328\u001b[0m                     \u001b[1;32mbreak\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1329\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1330\u001b[1;33m         \u001b[0mvocabulary\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_count_vocab\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mraw_documents\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfixed_vocabulary_\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1331\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1332\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbinary\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_count_vocab\u001b[1;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[0;32m   1199\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mraw_documents\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1200\u001b[0m             \u001b[0mfeature_counter\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1201\u001b[1;33m             \u001b[1;32mfor\u001b[0m \u001b[0mfeature\u001b[0m \u001b[1;32min\u001b[0m \u001b[0manalyze\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1202\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1203\u001b[0m                     \u001b[0mfeature_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mvocabulary\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mfeature\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36m_analyze\u001b[1;34m(doc, analyzer, tokenizer, ngrams, preprocessor, decoder, stop_words)\u001b[0m\n\u001b[0;32m    106\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    107\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mdecoder\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 108\u001b[1;33m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    109\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0manalyzer\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[0mdoc\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py\u001b[0m in \u001b[0;36mdecode\u001b[1;34m(self, doc)\u001b[0m\n\u001b[0;32m    224\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    225\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32mis\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnan\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 226\u001b[1;33m             raise ValueError(\n\u001b[0m\u001b[0;32m    227\u001b[0m                 \u001b[1;34m\"np.nan is an invalid document, expected byte or unicode string.\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m             )\n",
      "\u001b[1;31mValueError\u001b[0m: np.nan is an invalid document, expected byte or unicode string."
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tf_idf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "X_train_tf_idf = tf_idf_vectorizer.fit_transform(X_train)\n",
    "X_test_tf_idf = tf_idf_vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3adab9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_tf_idf.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07bda41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(X_train_tf_idf.toarray(), columns = tf_idf_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f05986a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c964eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix,classification_report, f1_score, recall_score, accuracy_score, precision_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c49ca992",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval(model, X_train, X_test):\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_pred_train = model.predict(X_train)\n",
    "    \n",
    "    print(\"Test_Set\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "    print(\"Train_Set\")\n",
    "    print(classification_report(y_train, y_pred_train))\n",
    "    fig, ax = plt.subplots(figsize=(8, 8))\n",
    "    \n",
    "    plot_confusion_matrix(model, X_test, y_test, ax=ax)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ee89b6",
   "metadata": {},
   "source": [
    "# 8) MACHINE & DEEP LEARNING MODELLING"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d5eacfe",
   "metadata": {},
   "source": [
    "## 8.1 Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35b18f14",
   "metadata": {},
   "source": [
    "### 8.1.a Logistic Regression With Count Vectorizor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0be684d0",
   "metadata": {},
   "source": [
    "### 8.1.b Logistic Regression With TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e86eb50",
   "metadata": {},
   "source": [
    "## 8.2 Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ab5075a",
   "metadata": {},
   "source": [
    "### 8.2.a Naive Bayes With Count Vectorizor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78cef682",
   "metadata": {},
   "source": [
    "### 8.2.b Naive Bayes With TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1867c0d4",
   "metadata": {},
   "source": [
    "## 8.3 Support Vector Machine (SVM)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00de1538",
   "metadata": {},
   "source": [
    "### 8.3.a Support Vector Machine (SVM) With Count Vectorizor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0a5e43",
   "metadata": {},
   "source": [
    "### 8.3.b Support Vector Machine (SVM) With TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "769715ab",
   "metadata": {},
   "source": [
    "## 8.4 LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6eb3a24",
   "metadata": {},
   "source": [
    "### 8.4.a LSTM With Count Vectorizor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf19776d",
   "metadata": {},
   "source": [
    "### 8.4.b LSTM With TF-IDF Vectorizer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
